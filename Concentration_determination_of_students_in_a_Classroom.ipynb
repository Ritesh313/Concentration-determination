{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Concentration_determination_of_students_in_a_Classroom.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritesh313/Concentration-determination/blob/master/Concentration_determination_of_students_in_a_Classroom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbhpRyi6vDZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This line of the code is to remove anything from the colab directory\n",
        "#!rm -rf /content/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMm5V7apvJu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1086803e-c9b2-4330-8f32-d06ec6c513ea"
      },
      "source": [
        "#This is the code to import data from google drive\n",
        "#Source - https://github.com/ndrplz/google-drive-downloader\n",
        "!pip install googledrivedownloader\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "#I imported the zip folder containing all the files I need in the code.\n",
        "gdd.download_file_from_google_drive(file_id='18SqzsPjkhoTpf6F-cHl4OhLQBXVjIsCj',\n",
        "                                    dest_path='/content/data.zip',\n",
        "                                    unzip=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n",
            "Downloading 18SqzsPjkhoTpf6F-cHl4OhLQBXVjIsCj into /content/data.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1crbv6Jatj1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is the section to divide the video into frames.\n",
        "import cv2 #Used to extract frames from video\n",
        "import os #Used to create directories\n",
        "frames='frames'\n",
        "if not os.path.exists(frames):\n",
        "  os.makedirs(frames) #This will create a folder named frames in the content directory of colab\n",
        "\n",
        "#Path to video file imported in the above section\n",
        "path = '/content/frames'\n",
        "vid = cv2.VideoCapture('/content/Project files/VID_20200426_122440.mp4') \n",
        "\n",
        "# success variable checks whether frames were extracted \n",
        "success = 1\n",
        "while success: \n",
        "  # vid is a cv2 object so, can be used to read frame \n",
        "  # function extract frames \n",
        "  success, image = vid.read()\n",
        "  if(success==False):\n",
        "    break\n",
        "  timestamps = vid.get(cv2.CAP_PROP_POS_MSEC)/1000 #cv2.CAP_PROP_POS_MSEC -  This is the function to find the timestamp (in ms) of the frame.\n",
        "  cv2.imwrite(os.path.join(path, \"%d.jpg\" % timestamps), image) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vP6tr4Bw5Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Source: https://github.com/omar178/Emotion-recognition.git\n",
        "#I used the pretrianed model from the above link\n",
        "#But I changed the code to perform experiments on which emotion falls under focused and which under distracted. \n",
        "#Also, I changed the code to insert the data into a csv file\n",
        "!git clone https://github.com/omar178/Emotion-recognition.git\n",
        "from natsort import natsorted, ns\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import imutils\n",
        "import cv2\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# These are the in-built models\n",
        "detection_model_path = '/content/Emotion-recognition/haarcascade_files/haarcascade_frontalface_default.xml'\n",
        "emotion_model_path = '/content/Emotion-recognition/models/_mini_XCEPTION.102-0.66.hdf5'\n",
        "\n",
        "face_detection = cv2.CascadeClassifier(detection_model_path) \n",
        "emotion_classifier = load_model(emotion_model_path, compile=False) # Loading the model\n",
        "\n",
        "EMOTIONS = [\"angry\" ,\"disgust\",\"confused\", \"happy\", \"gloomy\", \"surprised\",\"neutral\"] #These are the total 7 emotions\n",
        "Distracted =[\"angry\" ,\"disgust\",\"confused\",\"gloomy\", \"surprised\"] #The 5 emotions which I found that fall under Distracted by experimentation\n",
        "Concentrated=[\"happy\",\"neutral\"] #These 2 emotions fall under Concentrated\n",
        "\n",
        "with open('Concentration_log.csv', 'w') as file1: #This is for the csv file\n",
        "  csv_writer=csv.writer(file1)\n",
        "  csv_writer.writerow([\"Timestamp(seconds)\",\"Facial Expression\",\"Drowsy\",\"Concentration\"]) #This writes the header in the csv file\n",
        "  for path1 in natsorted(glob.glob('/content/frames/*.jpg'), key=lambda y: y.lower()): #Here I have used natsort to naturally sort the frames in ascending order.\n",
        "    frame = cv2.imread(path1)\n",
        "    #Reading the frame\n",
        "    frame = imutils.resize(frame,width=300)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    #The above line extracts faces from the frame\n",
        "    for face in (faces):\n",
        "      canvas = np.zeros((250, 300, 3), dtype=\"uint8\") #This is for creating the canvas you see in the output image.\n",
        "      frameClone = frame.copy()\n",
        "      (fX, fY, fW, fH) = face\n",
        "      # Extract the Regions Of Interest of the face from the grayscale image\n",
        "      # Use the ROI for classification via the CNN\n",
        "      roi = gray[fY:fY + fH, fX:fX + fW]\n",
        "      roi = cv2.resize(roi, (64, 64))\n",
        "      roi = roi.astype(\"float\") / 255.0\n",
        "      roi = img_to_array(roi)\n",
        "      roi = np.expand_dims(roi, axis=0)\n",
        "      preds = emotion_classifier.predict(roi)[0]\n",
        "      emotion_probability = np.max(preds)\n",
        "      label = EMOTIONS[preds.argmax()] #This is the final emotion prediction\n",
        "\n",
        "      #Below I am extracting the timetamp from the name of the image.\n",
        "      time=os.path.splitext(os.path.basename(path1))[0]\n",
        "      #writing to csv_file\n",
        "      if(label in Concentrated):\n",
        "        state_final=\"Concentrated\"\n",
        "      else:\n",
        "        state_final=\"Distracted\"\n",
        "      csv_writer.writerow([time,label,\"NO\",state_final])\n",
        "      for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
        "        #Text to be printed\n",
        "        text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
        "        # Drawing the Emotion + Probability index\n",
        "        w = int(prob * 300)\n",
        "        cv2.rectangle(canvas, (7, (i * 35) + 5),(w, (i * 35) + 35), (0, 0, 255), -1)\n",
        "        cv2.putText(canvas, text, (10, (i * 35) + 23),cv2.FONT_HERSHEY_SIMPLEX, 0.45,(255, 255, 255), 2)\n",
        "        cv2.putText(frameClone, label, (fX, fY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "        cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
        "      cv2_imshow(frameClone) #This is for displaying the final image\n",
        "      cv2_imshow(canvas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igA8AsOovf8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Source: https://www.pyimagesearch.com/2017/05/08/drowsiness-detection-opencv/\n",
        "#I used the pretrianed model from the above link\n",
        "#But I performed experiments to determine the threhold value for the aspect ratio. \n",
        "#Also, I changed the code to insert the data into a csv file\n",
        "\n",
        "from scipy.spatial import distance\n",
        "from imutils import face_utils\n",
        "import numpy as np\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import imutils\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "from natsort import natsorted, ns\n",
        "\n",
        "#Minimum threshold of eye aspect ratio below which alarm is triggerd\n",
        "EYE_ASPECT_RATIO_THRESHOLD = 0.2\n",
        "\n",
        "#Load face cascade which will be used to draw a rectangle around detected faces.\n",
        "face_cascade = cv2.CascadeClassifier(\"/content/Project files/haarcascade_frontalface_default.xml\")\n",
        "\n",
        "#This function calculates and return eye aspect ratio\n",
        "def eye_aspect_ratio(eye):\n",
        "  A = distance.euclidean(eye[1], eye[5])\n",
        "  B = distance.euclidean(eye[2], eye[4])\n",
        "  C = distance.euclidean(eye[0], eye[3])\n",
        "\n",
        "  ear = (A+B) / (2*C)\n",
        "  return ear\n",
        "\n",
        "#Load face detector and predictor, uses dlib shape predictor file\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"/content/Project files/shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "#Extract indexes of facial landmarks for the left and right eye\n",
        "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS['left_eye']\n",
        "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS['right_eye']\n",
        "\n",
        "for path1 in natsorted(glob.glob('/content/frames/*.jpg'), key=lambda y: y.lower()):\n",
        "  #Read each frame and flip it, and convert to grayscale\n",
        "\n",
        "  frame=cv2.imread(path1)\n",
        "  frame = imutils.resize(frame, width=300)\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  #Detect facial points through detector function\n",
        "  faces = detector(gray, 0)\n",
        "\n",
        "  #Detect faces through haarcascade_frontalface_default.xml\n",
        "  face_rectangle = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "  #Draw rectangle around each face detected\n",
        "  for (x,y,w,h) in face_rectangle:\n",
        "    cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "  #Detect facial points\n",
        "  for face in faces:\n",
        "    shape = predictor(gray, face)\n",
        "    shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "    #Get array of coordinates of leftEye and rightEye\n",
        "    leftEye = shape[lStart:lEnd]\n",
        "    rightEye = shape[rStart:rEnd]\n",
        "\n",
        "    #Calculate aspect ratio of both eyes\n",
        "    leftEyeAspectRatio = eye_aspect_ratio(leftEye)\n",
        "    rightEyeAspectRatio = eye_aspect_ratio(rightEye)\n",
        "\n",
        "    eyeAspectRatio = (leftEyeAspectRatio + rightEyeAspectRatio) / 2\n",
        "\n",
        "    #Use hull to remove convex contour discrepencies and draw eye shape around eyes\n",
        "    leftEyeHull = cv2.convexHull(leftEye)\n",
        "    rightEyeHull = cv2.convexHull(rightEye)\n",
        "    cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "    cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\n",
        "    #Detect if eye aspect ratio is less than threshold\n",
        "    if(eyeAspectRatio < EYE_ASPECT_RATIO_THRESHOLD):\n",
        "      cv2_imshow(frame)\n",
        "      time=int(os.path.splitext(os.path.basename(path1))[0])\n",
        "      ### This is for editing the csv file\n",
        "      f = open('/content/Concentration_log.csv', 'r')\n",
        "      reader = csv.reader(f)\n",
        "      mylist = list(reader)\n",
        "      f.close()\n",
        "      mylist[time+1][2]='YES'\n",
        "      my_new_list = open('/content/Concentration_log.csv', 'w', newline = '')\n",
        "      csv_writer = csv.writer(my_new_list)\n",
        "      csv_writer.writerows(mylist)\n",
        "      my_new_list.close()\n",
        "      ### csv file edited\n",
        "      print(\"Drowsy\")\n",
        "      \n",
        "    else:\n",
        "      continue\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQr7A7zw13g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This piece of code is to use the drowsiness determinator to find the concentration\n",
        "#Facial expression is not the only reliable pointer in case the person is drowsy. \n",
        "#Hence, the cocentration will be changed if the person is drowsy\n",
        "\n",
        "f = open('/content/Concentration_log.csv', 'r')\n",
        "reader = csv.reader(f)\n",
        "mylist = list(reader)\n",
        "\n",
        "for i in range(1,(len(mylist))):\n",
        "  if(mylist[i][2]==\"YES\"):\n",
        "    mylist[i][3]=\"Distracted\"\n",
        "f.close()\n",
        "\n",
        "my_new_list = open('/content/Concentration_log.csv', 'w', newline = '')\n",
        "csv_writer = csv.writer(my_new_list)\n",
        "csv_writer.writerows(mylist)\n",
        "my_new_list.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}